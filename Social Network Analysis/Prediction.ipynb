{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction -- Apply h2oAutoML to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yx921121/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import webhoseio\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "import csv\n",
    "\n",
    "from hatesonar import Sonar\n",
    "sonar = Sonar()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_vocab = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Train Dataset from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using TF-IDF Vector Transformation in Test Dataset required a train dataset, so we query train dataset from database again\n",
    "client = MongoClient('mongodb://May:7184393@pipeline-shard-00-00-jpov0.mongodb.net:27017,pipeline-shard-00-01-jpov0.mongodb.net:27017,pipeline-shard-00-02-jpov0.mongodb.net:27017/test?ssl=true&replicaSet=Pipeline-shard-0&authSource=admin&retryWrites=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\May Xiao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pipeline']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = client.test\n",
    "collection = db.pipeline\n",
    "db.collection_names(include_system_collections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39741"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = db['pipeline']\n",
    "posts.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter train and test with Hate Speech Library and Action Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#same preprocess did before\n",
    "hate_speech_posts = []\n",
    "for post in posts.find():\n",
    "    a = post.get('text')\n",
    "    ab = sonar.ping(text=a)\n",
    "    if ab.get('top_class') == 'hate_speech' and ab.get('classes')[0].get('confidence') >= 0.5:\n",
    "        hate_speech_posts.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abuse', 'assault', 'alarm', 'alert', 'ambush', 'ammunition', 'anguish', 'annihilate', 'arrest', 'assassinate', 'assault', 'attack', 'barrage', 'barricade', 'battle', 'beat', 'blackmail', 'blast', 'blindside', 'blow up', 'bomb', 'bombard', 'brawl', 'breach', 'bullet', 'bunker', 'burn', 'bury', 'caution', 'char', 'catch', 'chase', 'cheat', 'chop', 'combat', 'commit', 'conduct', 'conflagrate', 'conspire', 'conquer', 'counterattack', 'damage', 'dash', 'defend', 'demolish', 'destroy', 'deter', 'detonate', 'devastate', 'direct', 'disarray', 'dominate', 'dscape', 'exonerate', 'execute', 'explode', 'expunge', 'extort', 'ferment', 'feud', 'fight', 'firebomb', 'force', 'fright', 'garrison', 'grenade', 'guard', 'gun', 'hammer', 'harm', 'hijack', 'hit', 'ignite', 'incite', 'inflame', 'interdict', 'intervene', 'intimidate', 'invade', 'ire', 'jeer', 'kidnap', 'kill', 'knife', 'lynch', 'maim', 'manacle', 'maraud', 'massacre', 'menace', 'murder', 'neutralize', 'nitrate', 'overthrow', 'penalize', 'penetrate', 'persecute', 'pistol', 'plunder', 'pound', 'prey', 'prison', 'prohibit', 'prosecute', 'provocate', 'pulverize', 'raid', 'rape', 'ravage', 'ravish', 'rebel', 'reject', 'resist', 'retaliate', 'revenge', 'rifle', 'rift', 'riot', 'sabotage', 'sanction', 'savage', 'sentence', 'scare', 'scramble', 'screening', 'shoot', 'siege', 'skirmish', 'slash', 'slaughter', 'slay', 'stab', 'stalk', 'subdue', 'tank', 'terrorize', 'threaten', 'thwart', 'topple', 'torch', 'torpedo', 'torture', 'trample', 'trap', 'trespass', 'trigger', 'vandalize', 'vanish', 'victimize', 'violate', 'vitriol', 'war']\n"
     ]
    }
   ],
   "source": [
    "with open(\"action_words.csv\") as a:\n",
    "    reader = csv.reader(a, delimiter=\"\\t\")\n",
    "    action_words = list(reader)\n",
    "    action_words_list=[None]*len(action_words)\n",
    "    \n",
    "    for w in range(len(action_words)):\n",
    "        action_words_list[w]=action_words[w][0]\n",
    "    print (action_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threats_detections(text,terms):\n",
    "    sent_tokenize_list = sent_tokenize(text)\n",
    "    for i in sent_tokenize_list:\n",
    "        word_tokenize_list = word_tokenize(i)\n",
    "        tagged_list = nltk.pos_tag(word_tokenize_list)\n",
    "        \n",
    "        lemmatized_list = [None]*len(word_tokenize_list)\n",
    "        n=0\n",
    "        for w in word_tokenize_list:\n",
    "            if tagged_list[n][1][0] == 'V':\n",
    "                lemmatized_list[n]=wordnet_lemmatizer.lemmatize(w,pos='v')\n",
    "                n=n+1\n",
    "            else:\n",
    "                lemmatized_list[n]=wordnet_lemmatizer.lemmatize(w)\n",
    "                n = n+1\n",
    "\n",
    "        for word in lemmatized_list:\n",
    "            word = word.encode('ascii','ignore').lower()\n",
    "        \n",
    "        if \"I\" or \"I'll\" in lemmatized_list:\n",
    "            if \"will\" or \"would\" in lemmatized_list:\n",
    "                if len(set(terms)&set(lemmatized_list)) >=1:\n",
    "                    return 'yes'\n",
    "                    continue\n",
    "            elif \"be\" and \"go\" and \"to\" in lemmatized_list:\n",
    "                if len(set(terms)&set(lemmatized_list)) >=1:\n",
    "                    return 'yes'\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threaten_statement_T = {'threaten_statement':'True'}\n",
    "threaten_statement_F = {'threaten_statement':'False'}\n",
    "\n",
    "for s in range(len(hate_speech_posts)):\n",
    "    txt = hate_speech_posts[s]['text']\n",
    "    result = threats_detections(txt,action_words_list)\n",
    "    if result == 'yes':\n",
    "        hate_speech_posts[s].update(threaten_statement_T)\n",
    "    else:\n",
    "        hate_speech_posts[s].update(threaten_statement_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_intention =[]\n",
    "for i in range(len(hate_speech_posts)):\n",
    "    if hate_speech_posts[i]['threaten_statement'] == 'True':\n",
    "        mixed_intention.append(hate_speech_posts[i]['text'])\n",
    "\n",
    "#print(len(mixed_intention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaa = pd.DataFrame(mixed_intention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_list = [16,55,74,77,78,86,104,119,131,170,\\\n",
    "        178,197,205,217,236,254,256,258,309,312,373,385,393,\\\n",
    "        604, 611, 632,\\\n",
    "        691, 704]\n",
    "\n",
    "for i in range(len(aaa)):\n",
    "    if i in the_list:\n",
    "        aaa.at[i, 'y'] = 1\n",
    "    else:\n",
    "        aaa.at[i, 'y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = pd.read_csv('new_posts2.txt', sep='\\t', index_col=0)\n",
    "new1 = pd.DataFrame(new.loc[44])\n",
    "new1['y']= 1\n",
    "new1.columns = [0, 'y']\n",
    "\n",
    "new2 = pd.read_csv('intention_ww.txt',  index_col=0)\n",
    "new2['y']= 1\n",
    "new2.columns = [0, 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aaa1 = pd.concat([aaa,new1,new2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(aaa1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3975\n"
     ]
    }
   ],
   "source": [
    "no_intention =[]\n",
    "for i in range(len(hate_speech_posts)):\n",
    "    if hate_speech_posts[i]['threaten_statement'] == 'False':\n",
    "        no_intention.append(hate_speech_posts[i]['text'])\n",
    "\n",
    "print(len(no_intention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_no_intent = pd.DataFrame(no_intention)\n",
    "df_no_intent['y']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_1 = df_no_intent.iloc[:2782,:]\n",
    "test_1 = df_no_intent.iloc[2782:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_total = pd.concat([train,train_1],ignore_index=True)\n",
    "test_total = pd.concat([test,test_1],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train_total[0], train_total['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import New Data from Webhose to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YOUR_API_KEY = 03331b5c-5e5c-4bdf-baf8-edda165414a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read parameter from a txt file\n",
    "with open(\"search_terms.txt\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    search_terms = list(reader)\n",
    "\n",
    "format_str = '%m/%d/%Y'\n",
    "\n",
    "keyword = search_terms[0][1] \n",
    "language = search_terms[1][1] \n",
    "date = datetime.datetime.strptime(search_terms[2][1], format_str)\n",
    "unixtime = time.mktime(date.timetuple())\n",
    "site = search_terms[3][1]\n",
    "site_type = search_terms[4][1]\n",
    "\n",
    "# read webhose token from a txt file\n",
    "with open(\"webhose_api_token.txt\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    TOKEN = list(reader)\n",
    "\n",
    "webhoseio.config(TOKEN[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query data from webhose \n",
    "query_params = { \"q\": keyword,\\\n",
    "                \"site\": site,\\\n",
    "                \"site_type\": site_type,\\\n",
    "                 'ts':unixtime,\\\n",
    "                \"language\":language,\\\n",
    "                \"sort\": \"published\", \\\n",
    "                'accuracy_confidence': 'high',\\\n",
    "                'format':'json'}\n",
    "output = webhoseio.query(\"filterWebContent\", query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# webhose only output 100 rows once, to get more data, we let wenhose continue outputing data and concatenate them in a list\n",
    "output_next_list1 = []\n",
    "for b in output['posts']:\n",
    "    output_next_list1.append(b)\n",
    "    \n",
    "page_num = 1+int(output['totalResults']/100)\n",
    "\n",
    "n=0\n",
    "a=0\n",
    "\n",
    "while n<=page_num:\n",
    "    output_next = webhoseio.get_next()\n",
    "    for i in output_next['posts']:\n",
    "        output_next_list1.append(i)\n",
    "        a+=1\n",
    "    n+=1 \n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4852\n"
     ]
    }
   ],
   "source": [
    "#filter out text in posts is empty\n",
    "output_next_list = []\n",
    "for i in output_next_list1:\n",
    "    text = i.get('text')\n",
    "    if len(text) >= 2:\n",
    "        output_next_list.append(i)\n",
    "print (len(output_next_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we add a new attribute in dataset and divide them into three categories like we did in training a model\n",
    "post_category_neither = {'post_category':'neither'}\n",
    "post_category_offensive_language = {'post_category':'offensive_language'}\n",
    "\n",
    "cnt = 0\n",
    "hate_speech_posts = []\n",
    "for post in output_next_list:\n",
    "    a = post.get('text')\n",
    "    ab = sonar.ping(text=a)\n",
    "    if ab.get('top_class') == 'hate_speech' and ab.get('classes')[0].get('confidence') >= 0.5:\n",
    "        hate_speech_posts.append(post)\n",
    "        cnt += 1\n",
    "    elif ab.get('top_class') == 'hate_speech' and ab.get('classes')[0].get('confidence') < 0.5:\n",
    "        post.update(post_category_offensive_language)\n",
    "    elif ab.get('top_class') == 'offensive_language':\n",
    "        post.update(post_category_offensive_language)\n",
    "    elif ab.get('top_class') == 'neither':\n",
    "        post.update(post_category_neither)\n",
    "        \n",
    "#print (cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we store uuid in posts which is hate speech category in a list called hs_list\n",
    "threaten_statement_T = {'threaten_statement':'True'}\n",
    "threaten_statement_F = {'threaten_statement':'False'}\n",
    "hs_list = []\n",
    "for s in range(len(hate_speech_posts)):\n",
    "    txt = hate_speech_posts[s]['text']\n",
    "    result = threats_detections(txt,action_words_list)\n",
    "    if result == 'yes':\n",
    "        hate_speech_posts[s].update(threaten_statement_T)\n",
    "    else:\n",
    "        hate_speech_posts[s].update(threaten_statement_F)\n",
    "        hs_list.append(hate_speech_posts[s]['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "#we store text attribute of posts which is hate speech category in a list called mixed_intention\n",
    "mixed_intention =[]\n",
    "for i in range(len(hate_speech_posts)):\n",
    "    if hate_speech_posts[i]['threaten_statement'] == 'True':\n",
    "        mixed_intention.append(hate_speech_posts[i]['text'])\n",
    "\n",
    "print(len(mixed_intention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we store uuid attribute of posts which is hate speech category in a list called mixed_intention_uuid\n",
    "mixed_intention_uuid =[]\n",
    "for i in range(len(hate_speech_posts)):\n",
    "    if hate_speech_posts[i]['threaten_statement'] == 'True':\n",
    "        mixed_intention_uuid.append(hate_speech_posts[i]['uuid'])\n",
    "\n",
    "#print(len(mixed_intention_uuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we transform two lists into dataframe\n",
    "aaa = {'text':mixed_intention,'uuid':mixed_intention_uuid}\n",
    "df= pd.DataFrame(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we define test dataset\n",
    "X_test = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_only(item):\n",
    "    '''\n",
    "    Define basic tokenize function for reuse.\n",
    "    @itme(string): text\n",
    "    @return filtered_tokens(list of string):  tokenized text\n",
    "    '''\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    #tokens = [word.lower() for sent in nltk.sent_tokenize(item) for word in nltk.word_tokenize(sent)]\n",
    "    cd = [w for w in word_tokenize(item.lower())]\n",
    "    tokens = [word for word in cd if word not in stopwords_vocab]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', str(token)):\n",
    "            filtered_tokens.append(str(token))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(item):\n",
    "    '''\n",
    "    Define basic tokenize and stem function for reuse.\n",
    "    @itme(string): text\n",
    "    @return stems(list of string):  tokenized and stemmed text\n",
    "    '''\n",
    "    \n",
    "    stems = [stemmer.stem(t) for t in tokenize_only(item)]\n",
    "    return stems    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function is to transform the text column in train and test dataset into tfidfvector matrix\n",
    "#we use character vector and word vector at the same time to better extract info from a set of words\n",
    "def tfidf_vectorizer (train, test):\n",
    "    char_vector = TfidfVectorizer(max_df=0.9, max_features=100000,\\\n",
    "                                   min_df=0.01, stop_words='english',analyzer='char',\\\n",
    "                                   use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,4))\n",
    "    \n",
    "    word_vector = TfidfVectorizer(max_df=0.9, max_features=100000,\\\n",
    "                                   min_df=0.01, stop_words='english',analyzer='word',\\\n",
    "                                   use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,1))\n",
    "\n",
    "    train_char_matrix = char_vector.fit_transform(train)\n",
    "    test_char_matrix = char_vector.transform(test)\n",
    "    \n",
    "    train_word_matrix = word_vector.fit_transform(train)\n",
    "    test_word_matrix = word_vector.transform(test)\n",
    "    \n",
    "    train_tfidf_matrix = sparse.hstack([train_word_matrix, train_char_matrix])\n",
    "    test_tfidf_matrix = sparse.hstack([test_word_matrix, test_char_matrix])\n",
    "    \n",
    "    return train_tfidf_matrix, test_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\May Xiao\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#apply both train and test datasets to transform both into matrices\n",
    "X_train_vec, X_test_vec = tfidf_vectorizer(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3355, 6040)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 6040)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocess train and test to import them into h2o\n",
    "train_x = pd.DataFrame(X_train_vec.toarray())\n",
    "test_x = pd.DataFrame(X_test_vec.toarray())\n",
    "test_x.columns = test_x.columns.astype(str)\n",
    "train_x.columns = train_x.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Stored h2oAutoML to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)\n",
      "  Starting server from C:\\Users\\May Xiao\\Anaconda3\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\MAYXIA~1\\AppData\\Local\\Temp\\tmppwinuvcx\n",
      "  JVM stdout: C:\\Users\\MAYXIA~1\\AppData\\Local\\Temp\\tmppwinuvcx\\h2o_May_Xiao_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\MAYXIA~1\\AppData\\Local\\Temp\\tmppwinuvcx\\h2o_May_Xiao_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>21 days, 9 hours and 26 minutes </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_May_Xiao_oqvebs</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.524 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.0.2\n",
       "H2O cluster version age:    21 days, 9 hours and 26 minutes\n",
       "H2O cluster name:           H2O_from_python_May_Xiao_oqvebs\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.524 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "test = h2o.H2OFrame(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "#load saved model to predict\n",
    "model_path = 'auto_h2o_best\\DRF_1_AutoML_20181204_235019'\n",
    "saved_model = h2o.load_model(model_path)\n",
    "h2o_frame = saved_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_h2o = h2o_frame.as_data_frame()\n",
    "df_results = df.join(df_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in df_results.loc[df_results['predict'] == 0, 'uuid']:\n",
    "    hs_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th_list = []\n",
    "for i in df_results.loc[df_results['predict'] == 1, 'uuid']:\n",
    "    th_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store uuid labeled \"1\" posts in threatening statement category and labeled \"0\" posts in hate speech\n",
    "post_category_hate_speech = {'post_category':'hate_speech'}\n",
    "post_category_threatening_statement = {'post_category':'threatening_statement'}\n",
    "\n",
    "for i in output_next_list:\n",
    "    uid = i.get('uuid')\n",
    "    if uid in hs_list:\n",
    "        i.update(post_category_hate_speech)\n",
    "    elif uid in th_list:\n",
    "        i.update(post_category_threatening_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thread': {'uuid': 'a15245a7828f88cb0f4daee23bd4f7c3a4d80384', 'url': 'http://omgili.com/ri/_0JOtn.4SCoF5VTZjCccpVxqD8CQYo1pQBAIS8vnvo1SZKhoqvWNZO1X2OiBm6Yf', 'site_full': 'boards.4chan.org', 'site': '4chan.org', 'site_section': 'http://boards.4chan.org/pol/', 'site_categories': ['adult', 'non_standard_content'], 'section_title': '/pol/ - Politically Incorrect - 4chan', 'title': 'what is your honest opinion on race-mixing?', 'title_full': '/pol/ - what is your honest opinion on race-mixing? - Politically Incorrect - 4chan', 'published': '2018-12-08T23:34:00.000+02:00', 'replies_count': 232, 'participants_count': 2, 'site_type': 'discussions', 'country': 'US', 'spam_score': 0.001, 'main_image': '', 'performance_score': 0, 'domain_rank': 903, 'social': {'facebook': {'likes': 0, 'comments': 0, 'shares': 0}, 'gplus': {'shares': 0}, 'pinterest': {'shares': 0}, 'linkedin': {'shares': 0}, 'stumbledupon': {'shares': 0}, 'vk': {'shares': 0}}}, 'uuid': '00e30abbc99acc3d014413dff1b2513fb2a62f2f', 'url': 'http://omgili.com/ri/_0JOtn.4SCoF5VTZjCccpVxqD8CQYo1pQBAIS8vnvo1SZKhoqvWNZAoFjIZ8i7ka7jpIwktF7bg-', 'ord_in_thread': 44, 'author': 'Anonymous', 'published': '2018-12-08T23:53:00.000+02:00', 'title': '', 'text': '>>196047917\\nOh well, we will just have to kill you then, Jew.', 'highlightText': '', 'highlightTitle': '', 'language': 'english', 'external_links': [], 'entities': {'persons': [], 'organizations': [], 'locations': []}, 'rating': None, 'crawled': '2018-12-09T00:57:33.004+02:00', 'threaten_statement': 'True', 'post_category': 'threatening_statement'}\n",
      "{'thread': {'uuid': '005e8e6cf22170518d4374da616c8c0ee126ce5d', 'url': 'http://omgili.com/ri/_0JOtn.4SCoF5VTZjCccpVxqD8CQYo1pQBAIS8vnvo3psIYipwpPEXQ0Ec6K0okC', 'site_full': 'boards.4chan.org', 'site': '4chan.org', 'site_section': 'http://boards.4chan.org/pol/', 'site_categories': ['adult', 'non_standard_content'], 'section_title': '/pol/ - Politically Incorrect - 4chan', 'title': '>4 brick walls to look at >3 meals per day (that n', 'title_full': '/pol/ - >4 brick walls to look at >3 meals per day (that n - Politically Incorrect - 4chan', 'published': '2018-12-08T16:51:00.000+02:00', 'replies_count': 134, 'participants_count': 3, 'site_type': 'discussions', 'country': 'US', 'spam_score': 0.048, 'main_image': '', 'performance_score': 0, 'domain_rank': 903, 'social': {'facebook': {'likes': 0, 'comments': 0, 'shares': 0}, 'gplus': {'shares': 0}, 'pinterest': {'shares': 0}, 'linkedin': {'shares': 0}, 'stumbledupon': {'shares': 0}, 'vk': {'shares': 0}}}, 'uuid': '2d5b7e0dcdc6aa290c49bd528519705d560e19a7', 'url': 'http://omgili.com/ri/_0JOtn.4SCoF5VTZjCccpVxqD8CQYo1pQBAIS8vnvo3psIYipwpPERDE68GR1c0dcjM4_jyBr1g-', 'ord_in_thread': 47, 'author': 'Anonymous', 'published': '2018-12-08T18:00:00.000+02:00', 'title': '', 'text': '>>196006579\\nWe kill antifa now', 'highlightText': '', 'highlightTitle': '', 'language': 'english', 'external_links': [], 'entities': {'persons': [], 'organizations': [], 'locations': []}, 'rating': None, 'crawled': '2018-12-08T18:50:44.013+02:00', 'threaten_statement': 'True', 'post_category': 'threatening_statement'}\n",
      "{'thread': {'uuid': '278ffbfe5d240a556138ed1d11395c269cde94f1', 'url': 'http://omgili.com/ri/_0JOtn.4SCoF5VTZjCccpVxqD8CQYo1pQBAIS8vnvo3GTBnMJNxAlx5tmtI2B_JL', 'site_full': 'boards.4chan.org', 'site': '4chan.org', 'site_section': 'http://boards.4chan.org/pol/', 'site_categories': ['adult', 'non_standard_content'], 'section_title': '/pol/ - Politically Incorrect - 4chan', 'title': 'Deliberation started an hour ago', 'title_full': '/pol/ - Deliberation started an hour ago - Politically Incorrect - 4chan', 'published': '2018-12-07T17:55:00.000+02:00', 'replies_count': 242, 'participants_count': 1, 'site_type': 'discussions', 'country': 'US', 'spam_score': 0.0, 'main_image': '', 'performance_score': 0, 'domain_rank': 903, 'social': {'facebook': {'likes': 0, 'comments': 0, 'shares': 0}, 'gplus': {'shares': 0}, 'pinterest': {'shares': 0}, 'linkedin': {'shares': 0}, 'stumbledupon': {'shares': 0}, 'vk': {'shares': 0}}}, 'uuid': '90a9b0466f43c66ec854a274cbe3143d64cdff8e', 'url': 'http://omgili.com/ri/_0JOtn.4SCoF5VTZjCccpVxqD8CQYo1pQBAIS8vnvo3GTBnMJNxAl3951nBgd82bTZYOgbp2BJI-', 'ord_in_thread': 206, 'author': 'Anonymous', 'published': '2018-12-07T19:25:00.000+02:00', 'title': '', 'text': '>>195905815\\nHe hit people because he wasn\\'t going to stop in the middle of an Antifa mob wearing a \"white SUPREME-ist\" uniform while already being chased and attacked from behind. Why do you cocksuckers even lie? Just say you want to kill him for being racist.', 'highlightText': '', 'highlightTitle': '', 'language': 'english', 'external_links': [], 'entities': {'persons': [], 'organizations': [], 'locations': []}, 'rating': None, 'crawled': '2018-12-07T19:30:31.026+02:00', 'threaten_statement': 'True', 'post_category': 'threatening_statement'}\n"
     ]
    }
   ],
   "source": [
    "#show posts labeled as threatening statement\n",
    "for i in output_next_list:\n",
    "    if i.get('post_category') == 'threatening_statement':\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_bae8 closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.cluster().shutdown(prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output results to a json file to visulize in maltego\n",
    "with open('results.json', 'w') as fp:\n",
    "    json.dump(output_next_list, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
